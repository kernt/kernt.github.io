I need to run LDAP service on the Kubernetes cluster with some directory data imported. I choose OpenLDAP to run on the OpenShift Container Platform (OCP).

## Deploy OpenLDAP helm chart

With the Helm 3 release, the deployment of a helm chart is easy. There is an OpenLDAP chart available in the stable chart repo. We will use that.

Download the latest Helm3 tools, add the stable repo, and update it.

```sh
helm repo add stable https://kubernetes-charts.storage.googleapis.com/helm repo update
```

Create the following values.yaml file,

```
image:
  repository: osixia/openldap
  tag: latest
  # tag: 1.2.4

service:
  ldapPort: 389
  type: NodePort

env:
  LDAP_ORGANISATION: "Demo System"
  LDAP_DOMAIN: "demo.io.cpak"
  LDAP_TLS: "false"
  
persistence:
  enabled: true
  accessMode: ReadWriteOnce
  size: 8Gi
```

We will use the latest OpenLDAP docker image. Expose it through NodePort for external service to use. We enable the persistence to let K8s using the default dynamic storage class to provision.

We are ready to deploy the chart. Since we are running on OpenShift and the container needs to be running as root, we assign a proper Security Context Constraints (SCC), `anyuid`, to the service account of the namespace,

`oc adm policy add-scc-to-user anyuid -z default`

Deploy the chart,

`helm install my-ldap -f values.yaml stable/openldap -n my-ns`

Wait for the pods are ready, then get the admin password generated by the chart and do a ldapsearch,

```sh
export PASS=$(kubectl get secret -n my-ns my-ldap-openldap -o jsonpath="{.data.LDAP_ADMIN_PASSWORD}" | base64 -d)
export POD=$(oc get pods | grep my-ldap-openldap | awk '{print $1}')
oc exec -it $POD -- sh -c "ldapsearch -x -H ldap://localhost -b 'dc=demo,dc=io,dc=cpak' -D 'cn=admin,dc=demo,dc=io,dc=cpak' -w $PASS"
```

We have the initial directory data shown,

```
# extended LDIF  
#  
# LDAPv3  
# base <dc=demo,dc=io,dc=cpak> with scope subtree  
# filter: (objectclass=*)  
# requesting: ALL  
## demo.io.cpak  
dn: dc=demo,dc=io,dc=cpak  
objectClass: top  
objectClass: dcObject  
objectClass: organization  
o: Demo System  
dc: demo

# admin, demo.io.cpak
dn: cn=admin,dc=demo,dc=io,dc=cpak
objectClass: simpleSecurityObject
objectClass: organizationalRole
cn: admin
description: LDAP administrator
userPassword:: e1NTSEF9MHZ__SKIPPED__V1RGY1RGK0NiVmk=

# search result
search: 2  
result: 0 Success# numResponses: 3  
# numEntries: 2
```

Now we can import the LDIF data into the system.

## Update the OpenLDAP Schema

The LDIF data are exported from some system that custom schema was used. Before we can add the records into OpenLDAP, we need to update the schema.

The osixia/openldap container provides the schema conversion tool, we use that tool to convert the schema to a LDIF file.

First, validate the custom schema file to make sure the order and syntax are correct.

```
attributetypes ( 1.3.6.1.4.1.26027.1.1.2000 NAME 'lastLogin' DESC 'User last login timestamp' EQUALITY caseIgnoreMatch ORDERING caseIgnoreOrderingMatch SUBSTR caseIgnoreSubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.15 SINGLE-VALUE X-ORIGIN 'My Application' USAGE userApplications )
attributetypes ( 1.3.6.1.4.1.26027.1.1.2001 NAME 'createdDate' DESC 'Date when the account is created' EQUALITY caseIgnoreMatch ORDERING caseIgnoreOrderingMatch SUBSTR caseIgnoreSubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.15 SINGLE-VALUE X-ORIGIN 'My Application' USAGE userApplications )
attributetypes ( 1.3.6.1.4.1.26027.1.1.2002 NAME 'deletedDate' DESC 'Date when the account is deleted' EQUALITY caseIgnoreMatch ORDERING caseIgnoreOrderingMatch SUBSTR caseIgnoreSubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.15 SINGLE-VALUE X-ORIGIN 'My Application' USAGE userApplications )objectclass (  1.3.6.1.4.1.26027.1.2.2000 NAME 'MyPerson' SUP inetOrgPerson STRUCTURAL MAY ( lastLogin $ createdDate $ deletedDate ) X-ORIGIN 'My Application' )
```

Upload the schema file into the POD, launch a shell for the pod, run the convert tool script

```sh
kubectl cp my.schema $POD:/tmp/my.schema  
kubectl exec -it $POD -- sh
/container/service/slapd/assets/schema-to-ldif.sh /tmp/my.schema
```

The `schem-to-ldif.sh` covert my.schema into LDIF format named as my.ldif.

```
UTO-GENERATED FILE - DO NOT EDIT!! Use ldapmodify.
# CRC32 c34e13a0
dn: cn=my,cn=schema,cn=config
objectClass: olcSchemaConfig
cn: my
olcAttributeTypes: {0}( 1.3.6.1.4.1.26027.1.1.2000 NAME 'lastLogin' DESC 'Us
 er last login timestamp' EQUALITY caseIgnoreMatch ORDERING caseIgnoreOrderi
 ngMatch SUBSTR caseIgnoreSubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.
 15 SINGLE-VALUE X-ORIGIN 'My Application' )
olcAttributeTypes: {1}( 1.3.6.1.4.1.26027.1.1.2001 NAME 'createdDate' DESC '
 Date when the account is created' EQUALITY caseIgnoreMatch ORDERING caseIgn
 oreOrderingMatch SUBSTR caseIgnoreSubstringsMatch SYNTAX 1.3.6.1.4.1.1466.1
 15.121.1.15 SINGLE-VALUE X-ORIGIN 'My Application' )
olcAttributeTypes: {2}( 1.3.6.1.4.1.26027.1.1.2002 NAME 'deletedDate' DESC '
 Date when the account is deleted' EQUALITY caseIgnoreMatch ORDERING caseIgn
 oreOrderingMatch SUBSTR caseIgnoreSubstringsMatch SYNTAX 1.3.6.1.4.1.1466.1
 15.121.1.15 SINGLE-VALUE X-ORIGIN 'My Application' )
olcObjectClasses: {0}( 1.3.6.1.4.1.26027.1.2.2000 NAME 'MyPerson' SUP inetOr
 gPerson STRUCTURAL MAY ( lastLogin $ createdDate $ deletedDate ) X-ORIGIN '
 My Application' )
```

Based on this LDIF file, we can add it to the system.

`ldapadd -c -Y EXTERNAL -Q -H ldapi:/// -f /tmp/my.ldif`

The custom schema is loaded. Normal LDIF files can be imported then.